{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59e7560-64e1-4d0f-9a8c-6796daa6f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # model configuration\n",
    "    # base_model: str = \"ollama_chat/qwen2.5:1.5b-instruct-q8_0\"\n",
    "    # base_model: str = \"ollama_chat/llama3.2:1b-instruct-q8_0\"\n",
    "    # base_model: str = \"ollama_chat/exaone3.5:2.4b-instruct-q8_0\"\n",
    "    base_model: str = \"ollama_chat/granite3.1-dense:2b-instruct-q8_0\"\n",
    "    # base_model: str = \"ollama_chat/granite3.1-moe:3b-instruct-q8_0\"\n",
    "    temperature: float = 0.8\n",
    "    # teacher_model: str = \"openrouter/deepseek/deepseek-chat\"\n",
    "    # teacher_model: str = \"openrouter/meta-llama/Llama-3.3-70B-Instruct-Turbo\"\n",
    "    teacher_model: str = \"openrouter/qwen/qwen-2.5-72b-instruct\"\n",
    "    # teacher_model: str = \"openrouter/qwen/qwq-32b-preview\"\n",
    "    teacher_temperature: float = 0.8\n",
    "\n",
    "    reward_model: str = \"RLHFlow/Llama3.1-8B-PRM-Deepseek-Data\"\n",
    "\n",
    "    # dataset\n",
    "    dataset: str = \"HuggingFaceH4/MATH-500\"\n",
    "\n",
    "    # APIKEY (if using api for teacher)\n",
    "    api_key: str | None = None\n",
    "\n",
    "\n",
    "config = Config(\n",
    "    api_key = os.environ[\"OPENROUTER_APIKEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a9122b-c430-4264-bece-3d9abbfb6f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "# small, locally hosted base model\n",
    "lm = dspy.LM(config.base_model, api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# teacher model for instruction proposal\n",
    "teacher_lm = dspy.LM(config.teacher_model, api_key=config.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f52c9e5-35de-492e-89ed-1dd7c2d1dbe3",
   "metadata": {},
   "source": [
    "# Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee09d0-8f09-4ea9-ab2c-ab04add69546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prm import RewardEvaluator as RewardModel\n",
    "\n",
    "rm = RewardModel(\n",
    "    model_name=config.reward_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8339176-3c66-4d1e-b9c9-62a84c57bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from latex2sympy2 import latex2sympy\n",
    "from sympy import latex, simplify\n",
    "\n",
    "from src.utils.math import memoized_canonical_form\n",
    "from src.utils.qwen_math_parser import extract_answer\n",
    "from src.utils.qwen_grader import math_equal\n",
    "\n",
    "\n",
    "def is_valid_latex(expression: str) -> bool:\n",
    "    try:\n",
    "        latex2sympy(expression)\n",
    "    except:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def check_answer(example_answer, pred_answer) -> int:\n",
    "    a1 = memoized_canonical_form(example_answer)\n",
    "    a2 = memoized_canonical_form(pred_answer)\n",
    "    if a1 == a2:\n",
    "        return 1\n",
    "    else:\n",
    "        return int(math_equal(a1, a2))\n",
    "\n",
    "def _score(reward_model, example, pred, trace=None):\n",
    "    \"\"\"\n",
    "    Score the output\n",
    "    \"\"\"\n",
    "    print(f\"The answer: {example.answer} -> {pred.answer}\")\n",
    "    print(f\"The answer (repr): {repr(example.answer)} -> {repr(pred.answer)}\")\n",
    "    process_score = reward_model.evaluate(example.problem, pred)   \n",
    "    answer_score = check_answer(example.answer, pred.answer)\n",
    "    print(f\"Process: {process_score[-1]:.2f}, Answer {repr(pred.answer)}: {answer_score}\")\n",
    "    return (process_score[-1] + answer_score) / 2\n",
    "\n",
    "score = partial(_score, rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1a66d6-2f20-48c9-b625-99751e83ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dspy.Prediction(\n",
    "    problem=\"What is 2 + 3?\",\n",
    "    steps=[\n",
    "        \"Step 1: Identify the numbers to add: 2 and 3.\",\n",
    "        \"Step 2: Add the numbers together: 2 + 3 = 5.\"\n",
    "    ],\n",
    "    answer=\"5.\"\n",
    ")\n",
    "rm.evaluate(pred.problem, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0455b02-bf65-40ba-8795-2ea3d99d7e61",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e698e8-5b62-4009-8a95-2f618660cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load the MATH-500 dataset\n",
    "dataset = load_dataset(\"HuggingFaceH4/MATH-500\")\n",
    "split_dataset = dataset['test'].train_test_split(test_size=0.8, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']\n",
    "split_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# Inspect\n",
    "split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02da773c-5850-48d6-8984-0372e3348140",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e17ea1-cdc4-4926-bcca-644941c94fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "trainset = [dspy.Example(x).with_inputs(\"problem\") for x in split_dataset[\"train\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7365305-09dd-46f2-a734-9c2f1eec1773",
   "metadata": {},
   "source": [
    "# Best of N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a1e3a7-c37a-426c-9889-a26ded28d026",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae60ea-7be9-4724-9894-1635afd77db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_OF_N_PROMPT = \"\"\"\n",
    "Solve the following math problem efficiently and clearly:\n",
    "\n",
    "- For simple problems (2 steps or fewer)\n",
    "- For complex problems (3 steps or more)\n",
    "\n",
    "Steps should be very concise.\n",
    "Answer should be given in latex format for automatic evaluation using sympy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fe5fee-47f3-42e1-b0fc-06eac4fab82a",
   "metadata": {},
   "source": [
    "## Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbb3389-18ad-4d7e-b3f4-1c1e90ebe740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "class GenerateAnswerWithSteps(dspy.Signature):\n",
    "    __doc__ = BEST_OF_N_PROMPT\n",
    "    \n",
    "    problem: str = dspy.InputField(desc=\"A math problem to solve\")\n",
    "    steps: list[str] = dspy.OutputField(desc=\"An ordered list of steps that solve the problem.\")\n",
    "    solution: str = dspy.OutputField(desc=\"The solution to the problem.\")\n",
    "    answer: str = dspy.OutputField(desc=\"Only the final answer in latex, without extraneous parentheses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49c14b7-7c00-4d00-9f15-c829ce54e85b",
   "metadata": {},
   "source": [
    "## Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1cd228-2298-4644-ae95-43ee2038bb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.predictor = dspy.ChainOfThought(GenerateAnswerWithSteps)\n",
    "\n",
    "    def forward(self, problem: str):\n",
    "        pred = self.predictor(problem=problem)\n",
    "        dspy.Suggest(\n",
    "            is_valid_latex(pred.answer),\n",
    "            \"`answer` should be valid latex, and only the final answer.\",\n",
    "        )\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47d044f-0472-4817-b07f-b6c864801da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import MIPROv2\n",
    "\n",
    "teleprompter = MIPROv2(\n",
    "    metric=score,\n",
    "    auto=\"light\",\n",
    "    teacher_settings=dict(lm=teacher_lm),\n",
    "    num_threads=2\n",
    ")\n",
    "\n",
    "# predictor = dspy.ChainOfThought(GenerateAnswerWithSteps)\n",
    "predictor = GenerateAnswer().activate_assertions()\n",
    "optimized_program = teleprompter.compile(\n",
    "    student=predictor.deepcopy(),\n",
    "    teacher=predictor.deepcopy(),\n",
    "    trainset=trainset,\n",
    "    max_bootstrapped_demos=3,\n",
    "    max_labeled_demos=0,\n",
    "    requires_permission_to_run=False,\n",
    ")\n",
    "\n",
    "optimized_program.save(f\"mipro_optimized:{config.base_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209e5b3b-1a10-4cbb-87ce-801c4f9ece78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
